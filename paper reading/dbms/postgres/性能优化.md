## PostgreSQL性能优化

1. 操作系统优化：

   参考[Vonng大](http://vonng.com/)给出的tuned profile，以[OLTP配置](https://pigsty.cc/zh/docs/reference/kernel-optimize/#oltp%E9%85%8D%E7%BD%AE)为例：

   ```ini
   [cpu]
   governor=performance
   energy_perf_bias=performance
   min_perf_pct=100
   force_latency=1
   ```

   其中，`governor`指定了CPU的时钟频率，有以下选项：

   - performance: 尽可能使用最高频率
   - powersave: 尽可能使用最低频率
   - ondemand: 负载高时，用最高频率；空闲时，用最低频率
   - userspace: 允许用户态程序自己设定频率
   - conservatifve: 频率随负载动态调整

   `energy_perf_bias`指定了CPU在能耗和性能上的取舍，有以下选项：

   - performance: 不牺牲性能
   - normal: 允许牺牲较小的性能来节省能源
   - powersave: 最高效地节省能源

   `min_perf_pct`指定了CPU的[P-state](https://software.intel.com/content/www/us/en/develop/blogs/what-exactly-is-a-p-state-pt-1.html)的最小值，为最大化性能级别的百分比

   > **P-state**：是CPU的运行状态，即在不同电压和/或频率级别下运行的能力。
   >
   > **C-state**：是CPU的睡眠状态，即进入各种低功耗空闲状态的能力(具有不同的唤醒延迟)。

   `force_latency`强制设置了延迟时间为1秒（具体是在/dev/cpu_dma_latency）。

   ```ini
   [vm]
   # disable transparent hugepages
   transparent_hugepages=never
   ```

   其中，transparent_hugepages即启用/禁用Linux的透明大页。

   > Linux大页分为两类：
   >
   > - 标准大页 **Huge Page**
   >
   >   当大量内存被用于数据库时（像Oracle和PG这种用了共享内存来存储一些共享的资源，），将消耗大量内存来管理虚拟地址到物理地址的转换，即页表本身占的空间很大，因为要管理的页面很多，带来额外性能开销（表的entry太多，CPU的检索速度肯定也会受影响）。
   >
   >   在Linux 64位系统里面,默认内存是以4K的页面来管理的，1MB 内存等于 256 个页面。而Huge Page的默认值为2M（最高256MB），可以大大减小页表的大小。假设服务器内存64GB，共享内存大小32GB，如果是默认页面4KB，则有`32*1024*1024/4 = 8 388 608`个页表entry，假设每个entry占4字节，则页表自身约占`8M * 4B=32MB`，在考虑数据库并发连接有1024的话，则页表会占用到`32MB * 1024=32GB`的大小，就会发生OOM。
   >
   >   而且Huge Page管理的内存不会被Swap，避免了Swap引发的数据库性能问题；同时如果系统的内存非常大，也建议启用Huge Page。
   >
   >   建议：<u>大页大小 尽量和数据库需要的共享buffer一样大，否则多出来的部分也是浪费（大页内存不能用于其他用途）</u>。
   >
   >   优势如下：
   >
   >   - **Not swappable**: 减少了页面换入换出的开销
   >   - **Relief of TLB pressure**: 减轻了TLB的压力，因为TLB做为一个高速缓存，使用大页会减少cache miss，即带来更高的TLB命中率
   >   - **Decreased page table overhead**: 页表所占空间变小了
   >   - **Faster overall memory performance**: 避免原本访问页表的瓶颈
   >
   > - 透明大页 **Transparent Huge Page**
   >
   >   THP是运行时动态分配内存的，而Huge Page是在系统启动时预先分配内存的。[Oracle文档](https://docs.oracle.com/cd/E11882_01/install.112/e41961/memry.htm#CWLIN385)指出，THP运行时分配内存可能会带来额外的延迟。

   ```ini
   [sysctl]
   #-------------------------------------------------------------#
   #                           KERNEL                            #
   #-------------------------------------------------------------#
   # disable numa balancing
   kernel.numa_balancing=0
   
   # total shmem size in bytes： $(expr $(getconf _PHYS_PAGES) / 2 \* $(getconf PAGE_SIZE))
   {% if param_shmall is defined and param_shmall != '' %}
   kernel.shmall = {{ param_shmall }}
   {% endif %}
   
   # total shmem size in pages:  $(expr $(getconf _PHYS_PAGES) / 2)
   {% if param_shmmax is defined and param_shmmax != '' %}
   kernel.shmmax = {{ param_shmmax }}
   {% endif %}
   
   # total shmem segs 4096 -> 8192
   kernel.shmmni=8192
   
   # total msg queue number, set to mem size in MB
   kernel.msgmni=32768
   
   # max length of message queue
   kernel.msgmnb=65536
   
   # max size of message
   kernel.msgmax=65536
   
   kernel.pid_max=131072
   
   # max(Sem in Set)=2048, max(Sem)=max(Sem in Set) x max(SemSet) , max(Sem per Ops)=2048, max(SemSet)=65536
   kernel.sem=2048 134217728 2048 65536
   
   # do not sched postgres process in group
   kernel.sched_autogroup_enabled = 0
   
   # total time the scheduler will consider a migrated process cache hot and, thus, less likely to be remigrated
   # defaut = 0.5ms (500000ns), update to 5ms , depending on your typical query (e.g < 1ms)
   kernel.sched_migration_cost_ns=5000000
   ```

   内核参数中，`numa_balancing`是[NUMA自动平衡](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-numa-auto_numa_balancing)策略，若开启系统会自动的调配NUMA的内存使用，即会把任务移到与它们需要访问的内存更近的地方，同时也会移动任务所使用的数据到离他们更近的内存区域。可参考[Oracle的一个反馈](https://support.oracle.com/knowledge/Oracle%20Database%20Products/1053332_1.html)。

   `shmmax`和`shmall`分别指定了 <u>单个共享内存段的最大值</u>（以byte为单位） 和 <u>共享内存的总量</u>（以page为单位）。对于Oracle使用的SGA来说，官方建议`shmmax`应设置的足够大，多于物理内存的一半，可以取到 (物理内存大小-1byte)。但是从PG9.3后来主要使用[mmap类型的共享内存（而不是POSIX）](https://postgreshelp.com/postgresql-dynamic-shared-memory-posix-vs-mmap/)（POSIX相对SYSV是比较新的标准），所以设置一个较小的值，也可以正常启动。`shmmni`则指定的是共享内存段的最大数量。

   `msgmni`、`msgmnb`、`msgmax`分别指定了消息队列（系统内IPC）的最大消息队列数 、消息队列的最大字节数 、单条消息的最大字节数。可参考[RedHad文档](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-captun)。

   `sem`则是设置操作系统的信号量（可理解为进程之间的锁），4个数值分别为：

   - semmsl: 信号集的最大信号量数（ PG每16个进程一组, 每组需要17个信号量）
   - semmns: 整个系统的最大信号量数，等于 `semmsl * semmni`
   - semopm: semop函数在一次调用中所能操作一个信号量集中最大的信号量数（通常和semmsl相同）
   - semmni: 信号量集的最大数目，PG要求设置为数据库进程数的16分之一（包括postgres用户服务进程 以及 后台的工作进程）

   `sched_autogroup_enabled`则是开启/关闭Autogroup的特性，依据进程的类型，将不同的进程放到不同的组内，而调度的单位是组。[Linux文档](https://man7.org/linux/man-pages/man7/sched.7.html)指出 Autogroup特性更适合桌面交互（交互低的进程不会影响交互性高的进程）。

   `sched_migration_cost_ns`则用来判断一个进程是否仍为hot（若进程运行时间小于该变量，则认为是hot的，内核不会把它迁移出cache，所以这里需要**考虑query的执行时间**）。

   ```ini
   # 续[sysctl]部分
   #-------------------------------------------------------------#
   #                             VM                              #
   #-------------------------------------------------------------#
   # try not using swap
   vm.swappiness=0
   
   # disable when most mem are for file cache
   vm.zone_reclaim_mode=0
   
   # overcommit threshhold = 80%
   vm.overcommit_memory=2
   vm.overcommit_ratio=80
   
   # vm.dirty_background_bytes=67108864 # 64MB mem (2xRAID cache) wake the bgwriter
   vm.dirty_background_ratio=3       # latency-performance default
   vm.dirty_ratio=10                 # latency-performance default
   
   # deny access on 0x00000 - 0x10000
   vm.mmap_min_addr=65536
   ```

   其中，参数`swappiness`控制了使用swap分区的权重，值越大，使用swap就越积极，反之更倾向于使用物理内存。比如，值为100（即100%）时，操作系统会及时地把内存的数据换入swap分区中。如果设为0的话，就可以避免由于swap造成的性能下降（swap更能会占用IO，导致数据库性能下降）。

   `zone_reclaim_mode`是NUMA下的某个节点内存不足时的分配策略，即当前zone内存不够时，是**从下一个zone找** 还是**在当前本地zone内进行回收**。设置为0时，可以从其他zone或NUMA节点回收内存；设置为1时，会在当前本地zone内进行回收。由于本地zone回收的话，会回收很多cache，所以不希望cache被回收的话，就设为0比较好。（另外，设置为2的话，会将cache的脏页刷回硬盘；设置为4的话，会swap页面）

   `overcommit_memory`和`overcommit_ratio`这两个参数主要是控制内核对内存的超分配（即允许给进程申请的内存大小超过了实际可用的内存）。`overcommit_memory`有三种取值：

   - 0：默认值，允许合理的overcommit（通过算法来判断是否合理）
   - 1：允许任何overcommit
   - 2：禁止overcommit

   而`overcommit_ratio`就是用来判断是否overcommit了，[计算公式](https://access.redhat.com/solutions/665023)为`CommitLimit = 物理内存 * overcommit_ratio + SWAP大小`；若是用大页，`CommitLimit = (物理内存-大页TLB占的内存) * overcommit_ratio + SWAP大小`。

   `dirty_background_ratio`指定了内存中脏数据占空闲内存的百分比阈值，达到阈值后，会在后台异步地将脏数据刷到磁盘；而`dirty_ratio`指定了系统可填充脏数据的绝对百分比阈值，当达到该值时，会通过**pdflush**强制将内存中所有脏数据提交到磁盘，此时所有的IO都会被阻塞导致卡顿。[RedHat文档](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-tunables)推荐，数据库环境中，这两个值应设置较小，同时还可参考这篇[博客](https://lonesysadmin.net/2013/12/22/better-linux-disk-caching-performance-vm-dirty_ratio/)。

   `mmap_min_addr`参数指定用户进程通过mmap可使用的最小虚拟内存地址，以避免其在低地址空间产生映射导致安全问题。

   > 关于Linux内存管理对数据库的影响：https://engineering.linkedin.com/performance/optimizing-linux-memory-management-low-latency-high-throughput-databases

   ```ini
   # 续[sysctl]部分
   #-------------------------------------------------------------#
   #                        Filesystem                           #
   #-------------------------------------------------------------#
   # max open files: 382589 -> 167772160
   fs.file-max=167772160
   
   # max concurrent unfinished async io, should be larger than 1M.  65536->1M
   fs.aio-max-nr=1048576
   ```

   在文件系统部分中，`file-max`指定了**系统级别**所有进程可打开的文件总量。区别：指令`ulimit -n xxx`是**进程级别**能够打开的文件数量。

   `aio-max-nr`设置了异步IO请求数目的并发上限，在单机多实例的场景下，很容易达到默认值65536（也取决于数据库系统是否使用了异步IO，为了提高磁盘的操作性能，大部分DB都会采用异步IO）；而修改该值不会改变任何内核中数据结构的大小。

   ```ini
   # 续[sysctl]部分
   #-------------------------------------------------------------#
   #                          Network                            #
   #-------------------------------------------------------------#
   # max connection in listen queue (triggers retrans if full)
   net.core.somaxconn=65535
   net.core.netdev_max_backlog=8192
   # tcp receive/transmit buffer default = 256KiB
   net.core.rmem_default=262144
   net.core.wmem_default=262144
   # receive/transmit buffer limit = 4MiB
   net.core.rmem_max=4194304
   net.core.wmem_max=4194304
   
   # ip options
   net.ipv4.ip_forward=1
   net.ipv4.ip_nonlocal_bind=1
   net.ipv4.ip_local_port_range=32768 65000
   
   # tcp options
   net.ipv4.tcp_timestamps=1
   net.ipv4.tcp_tw_reuse=1
   net.ipv4.tcp_tw_recycle=0
   net.ipv4.tcp_syncookies=0
   net.ipv4.tcp_synack_retries=1
   net.ipv4.tcp_syn_retries=1
   
   # tcp read/write buffer
   net.ipv4.tcp_rmem="4096 87380 16777216"
   net.ipv4.tcp_wmem="4096 16384 16777216"
   net.ipv4.udp_mem="3145728 4194304 16777216"
   
   # tcp probe fail interval: 75s -> 20s
   net.ipv4.tcp_keepalive_intvl=20
   # tcp break after 3 * 20s = 1m
   net.ipv4.tcp_keepalive_probes=3
   # probe peroid = 1 min
   net.ipv4.tcp_keepalive_time=60
   
   net.ipv4.tcp_fin_timeout=5
   net.ipv4.tcp_max_tw_buckets=262144
   net.ipv4.tcp_max_syn_backlog=8192
   net.ipv4.neigh.default.gc_thresh1=80000
   net.ipv4.neigh.default.gc_thresh2=90000
   net.ipv4.neigh.default.gc_thresh3=100000
   
   net.bridge.bridge-nf-call-iptables=1
   net.bridge.bridge-nf-call-ip6tables=1
   net.bridge.bridge-nf-call-arptables=1
   
   # max connection tracking number
   net.netfilter.nf_conntrack_max=1048576
   ```

   在网络配置中，`somaxconn`参数指定了系统中每一个端口监听队列的最大长度，`netdev_max_backlog`参数则定义了INPUT端队列中数据包的最大数目（即网卡接收的数据包，内核还没处理）。

   `rmem_default`和`wmem_default`分别定义了TCP接受窗口 和 发送窗口的默认大小(Byte)；`rmem_max`和`wmem_max`分别定义了TCP接受窗口 和 发送窗口的最大大小(Byte)。

   [ipv4的子项](https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt)下

   - `ip_forward=1`即打开了系统的路由转发功能（主机有多块网卡时，可以根据包的目的ip地址将数据包发往本机另一块网卡，默认禁用）；`ip_nonlocal_bind=1`则允许绑定非本地的ip地址（在 HAproxy 及 Nginx 等应用中会用到，默认禁用）；`ip_local_port_range`定义了TCP/UDP协议允许使用的本地端口号范围。

   - `tcp_timestamps=1`启用TCP时间戳（会在TCP包头增加12个字节，RFC1323），能带来更高的性能（默认启用）；`tcp_tw_reuse=1`则允许将处于TIME-WAIT状态的socket用于新连接（从协议上安全时才能复用）；`tcp_tw_recycle=0`不允许更快地回收TIME-WAIT的socket；`tcp_syncookies=0`则关闭TCP的syn cookie（cookie作用为防止SYN Flood攻击，核心原理就是在完成三次握手前不会为任何一个连接分配任何资源，默认开启）；`tcp_synack_retries`设置了被动TCP连接尝试的SYNACK重传次数（如果**全连接**队列满了，且配置为扔掉Client的ACK的话，服务端将会直接忽略收到的ACK，然后不断尝试发送SYNACK）；`tcp_syn_retries`设置了主动TCP连接尝试的SYN重传次数（如果半连接满了之后的动作是直接忽略，此时作为客户端需要不断的重发SYN进行重试）；

     `tcp_keepalive_intvl`设置了KeepAlive的重发试探间隔；`tcp_keepalive_probes`指定了TCP需要发送多少个keepalive才能确定连接已经断开（默认为9）；`tcp_keepalive_time`指定了TCP多久发送一次keepalive消息（keepalive消息是一个简单的ACK，为 一个比当前seq小一的包，主机接收到这类ACK后，会返回一个包含当前seq的ACK每隔`tcp_keepalive_time`发送一次消息，如果没收到应答，每隔`tcp_keepalive_intvl`重发一次）。

     `tcp_fin_timeout`定义了本端断开的socket连接 维持FIN-WAIT-2的时间；`tcp_max_tw_buckets`指定了系统同时最多能有多少个TIMEWAIT状态的socket；`tcp_max_syn_backlog`指定了半连接的上限（即为SYN_RECV状态的连接，还没有获取对方的确认，这种半连接请求会放在队列中，注意和`somaxconn`的区别，`tcp_max_syn_backlog>=somaxconn`，且`somaxconn`为内核中的参数）。

     `neigh.default.gc_thresh`则是对于ARP高速缓存的配置。

   bridge子项下的参数则是 指定桥口流入的报文，经过arptable/iptable/ip6table来处理。

   `nf_conntrack_max`指定了nf_conntrack最多记录多少条连接。在iptable过滤包时，会使用状态跟踪机制，其中就是通过nf_conntrack模块来记录，可在 /proc/net/nf_conntrack中看到已经被track的连接。参考这篇博客《[你真的了解nf_conntrack么？](https://www.aikaiyuan.com/11300.html)》。

2. 数据库配置优化：

