## Distributed Transactions

1. 首先，对于单机数据库上的事务，其包含一个或多个数据库操作，但逻辑上构成一个整体；那么我们知道，这些操作要么全部执行成功，要么全部不执行。所以也有事务的 **ACID**（*Atomicity*, *Consistency*, *Isolation*, Durability） 特性。

   对于单机（单分区）事务，其一致性、隔离性主要是通过**并发控制**实现的；可以使用 基于锁的并发控制，比如两阶段锁；基于时间戳排序，也就是事务操作能否执行 取决于是否有更晚时间戳的事务已经提交。

2. 但是对于分布式事务，一个事务要跨越多个节点，单分区事务的并发控制方案 就不再适用。比如网购时候，订单系统和库存系统很可能就在不同的节点上，仅适用单分区事务的并发控制方案 无法得知本地外的其他节点事务是否成功提交。

   所以引入原子提交的概念：原子提交不允许参与者之间发生分歧，也就是只要有一个参与者投票反对，事务就不能提交。其本质就是对这个问题达成共识——**是否要执行当前被提议的事务**？

3. 两阶段提交

   两类节点

   - **协调者**：（一个）协调者负责保存状态，收集投票

   - **参与者**：其余节点即为参与者，通常每个参与者负责一个分区（互不相交的数据集合）

   两个阶段

   - **准备** *prepare*：*协调**者* Propose消息告诉*参与者* 新事务；*参与者* 如果可以提交自己那部分的事务，就投赞成票，否则投反对票中止事务。
   - **提交/中止** *commit/abort*：只要有任何一个*参与者* 投反对票中止事务，则*协调者* 会向所有*参与者* 发送Abort消息。也就是只有当参与者都投赞成票时，协调者 才向它们发送Commit消息。

4. 下面考虑几种故障场景

   首先考虑参与者，如果在准备阶段发生故障的话，则协调者会中止事务，因为没收到所有的赞成票，所以会影响可用性。像Spanner的话，为改进可用性问题，选择了在Paxos组上执行2PC 而非单节点。

   如果它是在接受了Propose后挂掉，则恢复后，需要follow协调者的最终决定（因为不知道其他节点是否会abort），在此之前，该参与者不能处理请求，否则可能会有数据不一致的情况。

   下面考虑协调者的故障。如果协调者在收集投票后、广播最终结果前发生故障，会导致所有参与者进入未决状态。因为参与者不知道协调者的决定，也不知道其他参与者是否已经收到了事务结果。那么解决方法 很自然，就是解决方案——在参与者中选举新的协调者，重新收集结果后，再做出最后决定。

   如果 协调者没能向发送所有参与者发送最终消息，就发生故障。则可以在参与者中选举新的协调者，询问那条事务的执行情况，做出决定。注意 **参与者的状态转化**：当参与者在投了赞同票 然后未能收到协调者commit/abort的消息(超时)后，它可以向其他参与者p<sub>i</sub>询问结果，有可能收到以下三种回复：

   - p<sub>i</sub>已收到了协调者的最终决定
   - p<sub>i</sub>在上一阶段投了反对票，即中止了
   - 同样也在等待最终决定（处于未决状态）=> 如果所有p都处于未决状态，那就有必要选举新的协调者

   但是如果 协调者和参与者一起故障，且故障的参与者接受到了最终消息并执行，那么在重新选举出新的协调者并决定时，可能造成数据不一致。就比如协调者向参与者1 发送rollback，然后挂掉了；参与者1 rollback成功后，也挂掉了；然后在参与者2 、3、4中选出新的协调者，由于他们都不知道那条事务的执行结果，所以重新投票后发现可以提交，从而导致了数据不一致

5. 这里需要讨论一下 两个概念 就是平常说的所谓分布式一致性算法 2PC 和 Paxos有什么区别，或者说他们是同一个类型、解决同一个问题的算法吗？

   首先，举一个分布式数据库的例子 https://www.zhihu.com/question/275845393/answer/385349629。分布式数据库一般采用数据的**partition**和**replication**来提高整体的与高可用特性。单机数据库的账户表中存储了A与B两个账户。在分布式数据库中，将A与B存储在不同节点，称为partition；对于每个数据数据保存多份副本（副本在不同节点），称为replication，如A的三个副本为A1、A2和A3，B的副本为B1、B2和B3。这样原先在一个节点的数据A与B，被分到了6个节点。

   那么从前面说的2PC中，我们也知道 两阶段提交 可以保证多个分片上操作的原子性，要么全部成功，要么全部失败；也就是A转钱给B，要么A-100同时B+100，要么都不变。而Paxos 或者 Raft，是用于保证同一个数据分片的多个副本的一致性，其在工程上的实现是**多个replication副本**对数据的操作序列达成一致。而且一个很明显的不同就在于，2PC中只有所有节点成功，整个事务才成功，但Paxos只要认为majority成功就行；这也体现出单纯的2PC不具备高可用，因为一个节点挂了，整个事务就阻塞住了，也就不可用了。

   关于这两者也有相应的学术名称，2PC成为**consistency**算法（也就是一致性），Paxos成为**consensus**算法（也就是共识算法），那是以为两者是一个完全互补的关系，就像之前说的Spanner在paxos组上进行两阶段提交？事实上不是，Lamport在*[Consensus on Transaction Commit](https://lamport.azurewebsites.net/video/consensus-on-transaction-commit.pdf)*这篇论文中就指出了，两阶段提交是基于Paxos提交的一个退化版情形。也就是说 完全可以在Paxos算法之上创建一个新算法来代替2PC，也有相应的论文**Paxos Commit**。

6. 三阶段提交的话，则把两阶段提交的准备阶段 划分为 提议+准备两个阶段，网上也有称为 CanCommit 和 PreCommit 的说法。

   - **提议** *propose*：*协调者* Propose消息告诉*参与者* 新事务并收集投票

   - **准备** *prepare*：如果投票通过，则*协调者* 发送Prepare消息，参与者会执行事务操作（写入undo和redo log），返回ACK；否则（比如有参与者投反对票，或参与者超时未回复）发送Abort消息并结束流程

   - **提交/中止** *commit/abort*：*协调者* 通知*参与者* 提交事务；如果协调者 没有在超时时间内收到所有ACK，则向所有参与者发送Abort，参与者利用undo log进行回滚

   **解决的问题**：

   解决了阻塞问题。比起2PC，3PC在参与者端额外引入了超时机制，从而如果参与者Prepare成功但无法及时收到来自协调者的信息，他会默认执行commit，而不会一直持有事务资源并处于阻塞状态。

   **未解决的问题**：

   发生**网络分区**的情况(有些地方也说 3PC是需要假设节点间的延迟与响应时间是有界的，这也是一个意思；如果延迟高到一定程度，那也等同于发生了网络分区)，Prepare阶段参与者节点A/B/C无法与协调者通信，导致A/B收到了prepare、但C没有；结果发生超时后，A/B提交了事务(因为A和B收不到协调者的Abort消息，按刚才的说法，超时后默认就commit了)，但C中止了，即产生了矛盾。

7. 前面我们提过，Spanner是在paxos共识组上进行两阶段提交，但是在真正深入了解之前，我发现很容易将几个事务实现概念弄混。就比如 之前提到的TrueTime，高精度的物理时钟API，它在事务管理中扮演的是什么角色？

   首先简单讲一下它的架构，这里的Zone可以理解为物理隔离的单位，每个zone里面有一个**zonemaster**和成百上千个**spanserver**，其中zonemaster负责给spanserver分配数据，spanserver才是真正处理用户请求的服务器；而location proxy顾名思义  会在客户向spanserver发送请求前，根据距离、延迟、负载等信息，决定访问哪个spanserver。

   深入来看spanserver，它是真正负责读写数据的地方，每个spanserver包含多个tablet，tablet可以理解为一个表的某些行，其数据结构也是kv的(key string, timestamp int)->string，(TiDB就是参照这套来搞)。同时，一个spanserver上的**每个tablet 都维护了一个Paxos状态机**，从而在每个paxos组中也会有一个leader，事实上这里paxos是进行的binlog的同步，也就是每个paxos提议的对象是binlog record。可以看到，每个leader上有lock table以及transaction manager，也就是现在所需要关注的对象。

   从Spanner的论文中可以知道，Lock table通过两阶段锁实现并发控制（悲观，为长事务设计，比如报表生成），Transaction Manager: 负责跨分片(跨Paxos group)的分布式事务，通过两阶段提交实现。其实这里可以把将一个个paxos group想像成单独的一个个节点，但是不能把paxos组的leader跟协调者弄混了，准确的说 只有leader节点会参与2PC，而协调者会由发起事务的client来决定。之前提到过两阶段提交存在单点故障问题，这里即使paxos leader宕机了，也可以很快用follower代替。

8. 那么之前提到的TrueTime到底是起什么作用？我们知道TrueTime它是用来生成全局时间戳的，而时间戳的本质就是为了建立事务的顺序关系。所以spanner所支持的三种事务类型中，只要是涉及时间戳的获取，就是由TrueTime API所支持的；并且，前面的tablet也提到是kv结构的，事实上key由两部分组成：一部分是字符串的键，另一部分就是该条记录的时间戳。那么为什么不能像TiDB那样 从 PD 获取一个全局唯一递增的时间戳作为当前事务的唯一事务 ID，这是因为 Spanner是跨全球的节点，引入一个单点的全局时间会带来很高的延迟影响。

   三种事务类型：

   - **读写事务**：需要加锁（悲观并发控制），以下考虑写操作：

     1. 确定写操作涉及的所有group，从中选取一个协调者

     2. 所有节点首先获取写锁

     3. 协调者生成本次事务的写入时间戳（需满足比之前任何事务的时间戳更大）

     4. Leaders(不管是不是协调者)将客户端提交的数据，通过paxos写入到副本binlog；注意：这里和一般的2PC不同，spanner的客户端是直接向对应的leader发送提交的数据，不管是协调者还是参与者；只不过参与者收到提交的数据后，会发送给协调者一个prepare回复
     5. 协调者收到所有参与者的prepare消息后，连带自己的Prepare以及Commit一起持久化，然后告知Client成功Commit
     6. 最后是通知参与者commit，并释放锁。当然这里的commit不仅仅是本地的commit，而是paxos组要进行commit

   - **只读事务**：无锁，可以在副本上进行；基于TrueTime实现了多版本的无锁读，即使当前的spanserver挂掉了，仍可以基于truetime从副本中读取

   - **快照读**：客户端可以指定快照读的时间戳 或 时间戳过期的时间点，无锁

9. Percolator 同样是 Google 提出的分布式事务解决方案，构建在 BigTable 上，像TiDB乐观事务的实现就是基于Percolator模型。

   在进行读写事务时，分以下几步：

   1. 在所有写操作的行中，选出**一行**作为Primary，其余涉及的行全称为secondary，在写入Lock列之前会检查是否已有锁；且在事务开始后，检查Write列是否有新的写操作已经提交(即发生冲突)
   2. Primary上好锁后，给secondary上锁，这里的锁会指向primary锁（也就是指向Account1）
   3. 提交阶段的CommitTs > StartTs，更新Write列，删除Lock列
   4. 如果Primary提交失败，整个事务回滚

10. TiDB 中事务 基于Percolator使用两阶段提交，分为 Prewrite 和 Commit 两个阶段；可以看到，在commit之前，在TiDB这边 buffer 所有的 update/delete 操作；但TiDB本身是无状态的，只能把最后的修改结果交给TiKV去判断冲突。

   并发事务频繁修改同一行时，乐观事务的性能由于冲突可能会很差，本质因为乐观事务 基于并发事务不常修改同一行的假设，从而跳过获取行锁来提升性能。

   所以TiDB后续也引入了悲观事务，当TiDB 收到来自客户端的更新数据的请求时，TiDB 向 TiKV 发起加悲观锁请求，该锁会持久化到 TiKV。当然，引入悲观锁的话，需要做好死锁的检测，终止特定的事务使整体能继续推进。

   在High Performance TiDB中，也提到了一下几点性能优化：

   - **基于的假设：**
     - 写冲突小
     - 需要恢复场景很少(但不能完全忽略)

   - **异步提交：**
     - 如果prewrite成功，可以直接返回client成功
     - 代价是恢复阶段更复杂，且需要保存所有的secondary key，内存占用会上升

   - Batch IO

   - 在MVCC中，对于占用空间小的值，直接保存在元信息的Lock或Write列（减少磁盘读写）

   

